Monday - 

Retention check:

Problem: You listed 15 elements for the σ-algebra in Q1b, missing {5}. Verify that {5} must be in the σ-algebra by showing
how to construct it from the generating sets {2,4,6} and {1,2,3} using complements, unions, and intersections only.

Answer: 

let F = {1,2,3} and G = {2,4,6}, F union G = A = {1,2,3,4,6} -> A compliment = {5}

Practive Problems:

Let X∼Uniform(0,1)X \sim \text{Uniform}(0,1)
X∼Uniform(0,1) and Y=X2Y = X^2
Y=X2.


What is E[Y∣σ(X)]\mathbb{E}[Y | \sigma(X)]
E[Y∣σ(X)]?

What is E[X∣σ(Y)]\mathbb{E}[X | \sigma(Y)]
E[X∣σ(Y)]?

Explain why one is trivial and one is not.


Let X1,X2X_1, X_2
X1​,X2​ be independent standard normals. Let G=σ(X1+X2)\mathcal{G} = \sigma(X_1 + X_2)
G=σ(X1​+X2​). Find E[X1∣G]\mathbb{E}[X_1 | \mathcal{G}]
E[X1​∣G]. *(Hint: symmetry + projection.)*

In 2–3 sentences, explain why E[X∣G]\mathbb{E}[X | \mathcal{G}]
E[X∣G] must be G\mathcal{G}
G-measurable. What would go wrong if it weren't?


Answers:
1.  a. if X = [0,1] and Y = X^2 E[Y|X] = X^2
    b. E[X|Y] = sqrt(Y)

2. if X1 and X2 have the same distribution, then E[X1|G]  = E[X2|G]. if G = X1 + X2, then can say E[X1 + X2|G] = X1 + X2. Therefore, E[x1 + x2|G] = 2E[X1|G] = X1 + X2 which leads us to the conclusion E[X1|G] = (1/2)(X1 + X2)

3. E[X|G] must be measurable because we want the expected value given some condition G, thats all the information available. If the expected value is not measurable, since G is, then the expected value would NOT be given if G, and thus not be valid, as you use information outside of G.


----------------------------------------------------------------------------------------------------------------------------------

Tuesday -

Problems:

(24-Hour Correction) Does Xn=n⋅1(0,1/n)→0X_n = n \cdot \mathbf{1}_{(0,1/n)} \to 0
Xn​=n⋅1(0,1/n)​→0 almost surely? In probability? Justify.

(48-Hour Review) Smallest σ-algebra containing {a,b}\{a,b\}
{a,b} on Ω={a,b,c,d}\Omega = \{a,b,c,d\}
Ω={a,b,c,d}.

Construct a sequence XnX_n
Xn​ that converges to 0 in probability but NOT almost surely. Explain why your example works.

Let Xn∼Bernoulli(1/n)X_n \sim \text{Bernoulli}(1/n)
Xn​∼Bernoulli(1/n). Does Xn→0X_n \to 0
Xn​→0 almost surely? In probability? In L1L^1
L1? Justify each.

Answers
1. That function describes a set vertical height / spike that as n increases, gets tinier and tiener in terms of width as 1/n apporaches zero, as X(w) = 1 for 0 < w < 1/n, and 0 everywhere else.
    - works for almost surely as increase n eventually will pick an outcome, w, that is greater than 1/n and as n goes to infinity, eventually 1 / n will go to 0
    - works for in probability as the probably = 1/n and as 1/n goes to infinity, the probabily of having 1 decreases eventually to zero

2. the smallest sigma-albrega set, F, is F = {empty set, omega, {a,b}, {c,d}}

3. Correct answer:

Classic example: Typewriter sequence on Ω=[0,1]\Omega = [0,1]
Ω=[0,1].

Define:
X1=1[0,1]
X2​=1[0,1/2]​, X3=1[1/2,1]
X4​=1[0,1/3]​, X5=1[1/3,2/3] X6=1[2/3,1]
And so on...

works for in probability because the widths of each interval shrinks such that P(X=1) goes to zero.
doesnt work for a.s. because we sweep across the whole [0,1] range, rather than a shrinking range, there is always a chance of X(w) = 1 in one of them

4.  P(X=1) = 1/n, and P(X) = 1 - 1/n in a Bernoulli random variable. so as n increases the probability of 1 decreases, similiarly the area under the curve decreases so in probabiliy and L^1 are a yes. a.s. is a no because ∑(n=,∞) ​P(Xn​=1)= ∑(n=1∞) ​1/n = infinity by Borell-cantelli, thus X(w) doesnt converge to 0


----------------------------------------------------------------------------------------------------------------------------------

Wednsday - 

Problems:
(48-Hour Review) Find E[X∣σ(X+Y)]\mathbb{E}[X | \sigma(X + Y)]
E[X∣σ(X+Y)] where X,YX, Y
X,Y are i.i.d. Uniform(0,1).

State the Strong Law of Large Numbers precisely. What mode of convergence does it guarantee?
Let X1,X2,...X_1, X_2, ...
X1​,X2​,... be i.i.d. with E[Xi]=5\mathbb{E}[X_i] = 5
E[Xi​]=5 and Var(Xi)=4\text{Var}(X_i) = 4
Var(Xi​)=4. Using the CLT, approximate P(Xˉ100>5.4)\mathbb{P}(\bar{X}_{100} > 5.4)
P(Xˉ100​>5.4).

In 2-3 sentences: Why does CLT give convergence in distribution rather than convergence in probability?


Answers:

1. X = Y thus by symmetry, E[X|X+Y] = E[Y|X+Y]

thus E[X+Y|X+Y] = X + Y

so E[X|sigma(X+Y)] is the same as E[X] + E[Y] where Y = X 

therefore 2E[X|simga(X+Y)] = X + Y = (X+Y) / 2

2. the strong law of large numbers (lln) states that as n apporaches infinity, the same mean will converge
on the population mean, or exepcted value of the population. It guarantees almost sure convergence as the probability of the same mean
approaching the population mean is 1.

3. mu = 5, var = 4, n = 100

since assume CLT, X ~ N(mu, var / n) thus var = 4/100 and std = sqrt(4/100) = .2

we can find the z - score, or how many stds a raw data point is from the mean to calculate the distance of the data point from the mean.

Z = (x - mu / std) therefore Z = 5.4 -5 / .2 = 2

using zscore lookup table, find that P(X_100 > 5.4) = .023


4. CLT, or the central limit theorem states that given enought independent random samples, the distribution will follow a normoal one.
CLT gives convergence in distribution because as number of samples, n, increase, then the probability of being far from the 
population mean decreases and approaches 0.







